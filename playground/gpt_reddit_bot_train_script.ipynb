{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Reddit API object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDITS = \"wallstreetbets\"\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=secrets.REDDIT_API_CLIENT_ID,\n",
    "    client_secret=secrets.REDDIT_API_CLIENT_SECRET,\n",
    "    user_agent=secrets.REDDIT_API_USER_AGENT\n",
    ")\n",
    "subreddit = reddit.subreddit(SUBREDDITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subreddit(display_name='wallstreetbets+investing+options+pennystocks+options+stocks')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top 1000 submissions from Wallstreetbets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975\n"
     ]
    }
   ],
   "source": [
    "top_submissions = []\n",
    "for submission in subreddit.top(limit=1000):\n",
    "    top_submissions.append(submission)\n",
    "\n",
    "print(len(top_submissions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over submissions to parse and fetch 10000 comments and replies in those threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from praw.models import MoreComments\n",
    "\n",
    "NUM_COMMENTS = 100000\n",
    "comments = 0\n",
    "conversations = []\n",
    "\n",
    "for submission in top_submissions:\n",
    "    for top_level_comment in submission.comments:\n",
    "        convo = []\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        convo.append(top_level_comment.body)\n",
    "        for reply in top_level_comment.replies:\n",
    "            if isinstance(reply, MoreComments):\n",
    "                continue\n",
    "            convo.append(reply.body)\n",
    "        \n",
    "        conversations.append(convo)\n",
    "        \n",
    "        comments = comments + 1\n",
    "        print(comments)\n",
    "    if comments > NUM_COMMENTS:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess conversations by turning text to lowercase and adding special spacing as per documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_conversations = []\n",
    "\n",
    "for convo in conversations:\n",
    "    the_convo = []\n",
    "    for comment in convo:\n",
    "        c = comment.lower().replace(\".\", \" .\").replace(\"?\", \" ?\").replace(\"!\", \" !\")\n",
    "        the_convo.append(c)\n",
    "    preprocessed_conversations.append(the_convo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten conversations so that a pool of all comments is available during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_convos = []\n",
    "\n",
    "for convo in preprocessed_conversations:\n",
    "    for comment in convo:\n",
    "        flat_convos.append(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Takes predefined number of comments and returns from flat convos so that random comments can be fed to the model during training\n",
    "def create_random_candidates():\n",
    "    candidates = []\n",
    "    for _ in range(3):\n",
    "        candidates.append(random.choice(flat_convos))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_of_utterances_by_words(utterances):\n",
    "    total_words = 0\n",
    "    for obj in utterances:\n",
    "        for candidate in obj[\"candidates\"]:\n",
    "            num_words = len(candidate.split(\" \"))\n",
    "            total_words = total_words + num_words\n",
    "        for history in obj[\"history\"]:\n",
    "            num_words = len(history.split(\" \"))\n",
    "            total_words = total_words + num_words\n",
    "    \n",
    "    return total_words\n",
    "\n",
    "def get_length_of_utterances_by_char(utterances):\n",
    "    total_words = 0\n",
    "    for obj in utterances:\n",
    "        for candidate in obj[\"candidates\"]:\n",
    "            num_words = len(candidate)\n",
    "            total_words = total_words + num_words\n",
    "        for history in obj[\"history\"]:\n",
    "            num_words = len(history)\n",
    "            total_words = total_words + num_words\n",
    "    \n",
    "    return total_words\n",
    "\n",
    "def get_length_of_personality_by_words(personality):\n",
    "    total_words = 0\n",
    "    for p in personality:\n",
    "        total_words = total_words + len(p.split(\" \"))\n",
    "    \n",
    "    return total_words\n",
    "\n",
    "def get_length_of_personality_by_char(personality):\n",
    "    total_words = 0\n",
    "    for p in personality:\n",
    "        total_words = total_words + len(p)\n",
    "    \n",
    "    return total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4688"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "MAX_TENSOR_SIZE = 350\n",
    "\n",
    "personality = [\n",
    "    \"i like stocks\",\n",
    "    \"i am from wallstreetbets\",\n",
    "    \"i like reddit\",\n",
    "    \"i love holding the stock\"\n",
    "]\n",
    "\n",
    "# Training data will have a specific format as specified in this article: https://towardsdatascience.com/how-to-train-your-chatbot-with-simple-transformers-da25160859f4\n",
    "training_data = []\n",
    "\n",
    "for convo in preprocessed_conversations:\n",
    "    the_convo = {}\n",
    "    utterances = []\n",
    "    convo_len = len(convo)\n",
    "    history = []\n",
    "    total_words = 0\n",
    "    if convo_len > 1:\n",
    "        for i, comment in enumerate(convo):\n",
    "            convo_obj = {}\n",
    "            if i % 2 == 0:\n",
    "                if convo_len >= i+2:\n",
    "                    len_words = len(comment) + len(convo[i+1])\n",
    "                    total_words = total_words + len_words\n",
    "                    if total_words > MAX_TENSOR_SIZE:\n",
    "                        continue # Just skip for now\n",
    "                    else:\n",
    "                        history.append(comment)\n",
    "                        convo_obj[\"candidates\"] = create_random_candidates()\n",
    "                        convo_obj[\"candidates\"].append(convo[i+1])\n",
    "                        convo_obj[\"history\"] = copy.deepcopy(history)\n",
    "                        utterances.append(convo_obj)\n",
    "                        total_words = get_length_of_utterances_by_char(utterances) + get_length_of_personality_by_char(personality)\n",
    "                        if len(utterances) > 0 and total_words < MAX_TENSOR_SIZE:\n",
    "                            the_convo[\"personality\"] = personality\n",
    "                            the_convo[\"utterances\"] = utterances\n",
    "                            training_data.append(the_convo)\n",
    "\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/minimal_train.json\", \"w\") as f:\n",
    "    json.dump(training_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.conv_ai import ConvAIModel\n",
    "\n",
    "train_args = {\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"use_early_stopping\": True\n",
    "}\n",
    "\n",
    "model = ConvAIModel(\"gpt\", \"gpt_personachat_cache\", use_cuda=True, args=train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(\"data/minimal_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "model = ConvAIModel(\"gpt\", \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no, we need to be smart enough to know\n",
      "no, but i can t help it\n",
      "that s what i want to know\n",
      "amc.\n"
     ]
    }
   ],
   "source": [
    "personality = [\n",
    "    \"i like stocks\",\n",
    "    \"i am from wallstreetbets\",\n",
    "    \"i like reddit\",\n",
    "    \"i love holding the stock\"\n",
    "]\n",
    "\n",
    "response, history = model.interact_single(message=\"should I buy meme stocks?\", history=[], personality=personality)\n",
    "print(response)\n",
    "response, history = model.interact_single(message=\"are you a millionaire?\", history=history, personality=personality)\n",
    "print(response)\n",
    "response, history = model.interact_single(message=\"what stock will go to the moon?\", history=history, personality=personality)\n",
    "print(response)\n",
    "response, history = model.interact_single(message=\"should I buy GME or AMC?\", history=history, personality=personality)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96d6f493f1618e22867629903e16fd82dd69d2bbf5bc5d07f594927da6b9a9e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('reddit-broker-bot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
